{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Il3ED-1Vj5Fh"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "QA_input = [{'question': 'How many programming languages does BLOOM support?',\n",
        "             'context' : 'BLOOM has 176 billion parameters, BLOOM is able to generate text in 46 natural languages and 13 programming languages.'},\n",
        "            {'question': 'Why is model conversion important?',\n",
        "             'context': 'The option to convert models between FARM and transformers gives freedom to the user and let people easily switch between frameworks.'},\n",
        "            {'question': 'What are the key responsabilities?',\n",
        "             'context': '''\n",
        "             Senior Embedded Software Engineer sought after by an expert in developing cutting-edge RF, microwave, and mmWave technologies.\n",
        "\n",
        "              As a Senior Embedded Software Engineer, you'll have the opportunity to develop intricate solutions for captivating endeavours that encompass sectors like Telecommunications Infrastructure, Critical Communications and Aerospace/Space.\n",
        "\n",
        "              If you are a Senior Embedded Software Engineer and sense that your creative potential is constrained, then this is the ideal opportunity for you. Here, you'll have the chance to engage in a diverse array of projects thanks to the multitude of global clients they serve.\n",
        "\n",
        "              Key Responsibilities:\n",
        "\n",
        "              Embedded C/C++\n",
        "              Bare-metal / Bootloader / U-Boot\n",
        "              RTOS/FreeRTOS\n",
        "\n",
        "              For security purposes, you must be eligible for SC Clearance\n",
        "\n",
        "              If you're interested in learning more about this exceptional opportunity, reach out to Danny Beecroft at IC Resources.'''}]"
      ],
      "metadata": {
        "id": "KbsfA4cmkLSW"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-XYLtLQvzm7n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'deepset/roberta-base-squad2'"
      ],
      "metadata": {
        "id": "HFkI13R6kOgx"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Method 1"
      ],
      "metadata": {
        "id": "czjvD59ZkThs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p9mZSZLqkVbx",
        "outputId": "184d4b58-2fd0-412e-db4f-0856b5b9f187"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs0 = tokenizer(QA_input[0]['question'], QA_input[0]['context'], return_tensors='pt')\n",
        "output0 = model(**inputs0)"
      ],
      "metadata": {
        "id": "Il_PNqwtkaDt"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs1 = tokenizer(QA_input[1]['question'], QA_input[1]['context'], return_tensors='pt')\n",
        "output1 = model(**inputs1)"
      ],
      "metadata": {
        "id": "hExdLjnpkdS5"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs2 = tokenizer(QA_input[2]['question'], QA_input[2]['context'], return_tensors='pt')\n",
        "output2 = model(**inputs2)"
      ],
      "metadata": {
        "id": "0z7ZbrFpquPW"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4pqFpmNtkf4Y",
        "outputId": "dc7acee6-d384-4154-ee8c-842487c22fa8"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "QuestionAnsweringModelOutput(loss=None, start_logits=tensor([[  2.1341,  -9.2678,  -9.0902,  -9.5148,  -9.9579,  -9.6713,  -9.5384,\n",
              "          -9.8904, -10.0625,  -9.6074,  -9.9357,  -9.7291,  -9.7478,  -5.3833,\n",
              "          -7.8231,  -8.5859,  -7.7111,  -2.4021,  -7.0713,  -6.8286,  -8.0475,\n",
              "          -3.7740,  -7.1794,  -8.2997,  -7.4724,  -6.7086,  -7.4467,  -6.3232,\n",
              "          -6.3538,  -5.9786,  -0.1657,  -5.1392,  -6.4773,  -5.0963,   7.0305,\n",
              "          -4.4435,  -4.7365,  -4.7513,  -9.5651]], grad_fn=<CloneBackward0>), end_logits=tensor([[ 2.6057, -8.2150, -7.7487, -8.0617, -7.2629, -7.9822, -8.2034, -7.7669,\n",
              "         -7.1772, -7.5019, -7.2047, -7.7352, -7.7530, -8.6603, -8.6695, -6.5531,\n",
              "         -8.8167, -3.1745, -6.0737, -5.6397, -6.1613, -7.8476, -8.0833, -5.3424,\n",
              "         -8.6863, -8.5810, -8.6701, -8.3157, -6.4040, -6.9607, -0.1846, -5.2936,\n",
              "         -2.9321, -6.6288,  7.4622, -0.8284,  2.5859,  0.1496, -8.0390]],\n",
              "       grad_fn=<CloneBackward0>), hidden_states=None, attentions=None)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "answer_start_idx = torch.argmax(output0.start_logits)\n",
        "answer_end_idx = torch.argmax(output0.end_logits)\n",
        "\n",
        "answer_tokens = inputs0.input_ids[0, answer_start_idx: answer_end_idx + 1]\n",
        "answer = tokenizer.decode(answer_tokens)\n",
        "print(\"question: {}\\nanswer: {}\".format(QA_input[0]['question'], answer))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LpdfoJKskkcL",
        "outputId": "2287c8df-3995-43a7-cc38-b740667fd5c3"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "question: How many programming languages does BLOOM support?\n",
            "answer:  13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6UYKg-2Aq3S-",
        "outputId": "e93257ad-a5f5-4f7b-c15d-bdba5f0fd14b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "QuestionAnsweringModelOutput(loss=None, start_logits=tensor([[ 2.4696, -8.7165, -9.3362, -9.1561, -8.6623, -8.5286, -9.6264, -9.8025,\n",
              "         -9.2953, -9.0738, -2.7862, -4.4684, -5.0018, -5.6218, -5.6525, -5.6242,\n",
              "         -5.5909, -5.7339, -5.9586, -6.0620, -6.0880, -6.0728, -5.7667, -3.7222,\n",
              "         -6.7972, -8.4619, -8.1601, -8.2489, -7.1260, -9.0762, -8.4882, -7.6637,\n",
              "         -7.2542, -8.1110, -5.3226, -4.2663, -8.1566, -7.5099, -5.3129, -8.6934,\n",
              "         -6.9070, -9.0393, -8.5723, -6.6231, -8.3778, -6.8358, -7.2430, -1.4065,\n",
              "         -3.9166, -4.6505, -5.6852, -5.7012, -5.6472, -5.6742, -5.8375, -5.9592,\n",
              "         -6.1104, -6.2380, -6.2929, -6.0424, -5.3851, -5.1946, -7.6683, -5.6413,\n",
              "         -7.5875, -8.6912, -8.4220, -8.6689, -9.0514, -5.7001, -8.3736, -7.1103,\n",
              "         -6.1159, -5.6111, -6.4596, -4.4880, -5.1615, -5.3911, -7.5417, -5.6260,\n",
              "         -8.0467, -6.9243, -8.3828, -8.4066, -7.8817, -5.8914, -8.8078, -4.5961,\n",
              "         -7.5314, -8.3544, -5.9047, -8.1234, -8.3838, -6.0053, -8.9935, -7.4550,\n",
              "         -7.9136, -2.3187, -4.1704, -4.8332, -5.8862, -5.9155, -5.8810, -5.8606,\n",
              "         -5.9436, -6.0920, -6.2731, -6.4094, -6.4534, -6.1459, -5.7363, -4.3784,\n",
              "         -6.9607, -8.5636, -7.4601, -5.2092, -7.4683, -8.5861, -8.1566, -8.4240,\n",
              "         -8.2017, -5.2214, -7.5148, -5.5068, -3.4986, -5.6779, -7.8891, -5.1221,\n",
              "         -9.1718, -8.0605, -7.5086, -8.3670, -7.5573, -6.6675, -8.2415, -8.3586,\n",
              "         -8.9199, -7.2434, -6.1332, -9.5200, -5.6404, -8.2440, -6.8667, -5.8553,\n",
              "         -5.5478, -6.4273, -3.9306, -7.0571, -4.8914, -3.8418, -6.0098, -7.6381,\n",
              "         -4.0842, -5.7738, -7.9383, -6.4282, -4.8746, -7.9835, -3.9772, -5.0915,\n",
              "         -7.5191, -7.1570, -7.2435, -0.8576, -1.5542, -3.1055, -4.7041, -5.1050,\n",
              "         -5.1522, -5.1791, -5.2076, -5.3488, -5.4347, -5.4437, -5.3831, -5.2800,\n",
              "         -4.6276, -2.5706, -3.9118, -6.8796, -4.8008, -0.0706, -1.4453, -2.8653,\n",
              "         -4.2681, -4.6404, -4.9298, -5.2223, -5.4212, -5.4700, -5.5582, -5.6030,\n",
              "         -5.5761, -4.9684, -3.8366,  2.6179, -4.2456, -2.3974, -6.0317, -5.0735,\n",
              "         -4.4713, -1.9578, -3.1365, -3.7895, -5.5168, -5.7316, -5.7988, -5.8531,\n",
              "         -5.9068, -5.9404, -6.0400, -6.1425, -6.1900, -6.0474, -5.7005, -1.3318,\n",
              "         -6.8931, -6.0153, -6.1328, -5.1611, -7.4741, -6.9260, -5.4071, -8.2014,\n",
              "         -6.7140, -2.9692, -3.8904, -4.3950, -5.7775, -6.0854, -6.1886, -6.2490,\n",
              "         -6.2745, -6.3218, -6.4012, -6.4915, -6.5399, -6.6589, -6.6321, -3.0361,\n",
              "         -7.7202, -6.8654, -6.4726, -7.1043, -7.8967, -3.3586, -4.5308, -5.3515,\n",
              "         -6.3919, -6.5794, -6.6675, -6.7272, -6.8186, -6.9339, -7.0309, -7.1222,\n",
              "         -7.1902, -6.9841, -6.6940, -3.9528, -4.4183, -9.2288, -9.4160, -6.3228,\n",
              "         -7.8589, -8.2342, -6.1999, -8.1555, -3.9287, -7.3976, -8.4569, -4.8147,\n",
              "         -4.3079, -5.0705, -6.4721, -6.6914, -6.7192, -6.7180, -6.7482, -6.7948,\n",
              "         -6.8651, -6.9772, -7.0742, -7.1049, -6.6356, -6.8040, -8.0026, -8.9760,\n",
              "         -8.1681, -8.8781, -7.9605, -8.9570, -8.8119, -8.1862, -7.2981, -8.7355,\n",
              "         -9.5583, -7.2030, -8.8816, -8.7189, -6.5102, -8.1260, -8.9129, -8.8571,\n",
              "         -9.4550, -8.5970, -6.5539, -8.9638, -7.2434, -9.3765]],\n",
              "       grad_fn=<CloneBackward0>), end_logits=tensor([[ 2.9109, -8.7628, -8.5367, -8.8316, -9.0372, -8.9619, -7.2958, -7.1747,\n",
              "         -8.3730, -8.1644, -2.9849, -5.1775, -5.5122, -5.6537, -5.5325, -5.3335,\n",
              "         -5.0843, -4.9355, -4.6123, -4.2941, -4.1027, -4.1001, -4.2773, -7.4812,\n",
              "         -8.6802, -6.0489, -6.8917, -3.4766, -8.6972, -6.9540, -8.7479, -9.0550,\n",
              "         -8.0537, -8.7498, -7.1480, -7.7868, -8.4523, -7.1063, -5.7575, -8.1816,\n",
              "         -5.6901, -8.1330, -8.4019, -7.8813, -6.0046, -2.6437, -2.8383, -1.2486,\n",
              "         -4.3886, -4.9283, -5.2042, -5.1090, -4.9448, -4.8096, -4.5852, -4.3444,\n",
              "         -4.2984, -4.1218, -3.9711, -3.7690, -3.6965, -9.4796, -9.3077, -8.5490,\n",
              "         -8.8302, -7.6620, -8.0490, -4.8957, -7.6944, -9.2263, -8.4804, -9.4360,\n",
              "         -9.1644, -7.4268, -8.5596, -6.8140, -7.2749, -3.9466, -7.7892, -8.3624,\n",
              "         -6.9736, -7.5552, -4.3859, -8.2698, -8.5192, -6.2889, -8.0456, -7.7170,\n",
              "         -5.7250, -8.0188, -8.3283, -6.0917, -8.2746, -6.2222, -7.2055, -3.7340,\n",
              "         -2.9674, -2.0475, -4.9980, -5.5277, -5.6225, -5.3637, -5.2605, -5.1864,\n",
              "         -5.1048, -4.9781, -4.8284, -4.6602, -4.4363, -3.9327, -4.0282, -8.9979,\n",
              "         -9.1870, -8.9545, -9.3429, -8.3967, -8.7300, -7.4802, -7.8219, -4.3396,\n",
              "         -8.8131, -8.4148, -9.0014, -9.0074, -5.8976, -4.6747, -8.3940, -3.2063,\n",
              "         -5.9674, -8.6470, -8.5661, -9.1188, -9.4766, -8.6095, -7.2119, -9.0137,\n",
              "         -6.9245, -2.8383, -7.9515, -7.8438, -9.0731, -8.5341, -9.4603, -9.1308,\n",
              "         -8.0619, -8.3965, -7.6124, -7.9201, -8.5070, -6.6188, -6.4704, -8.0161,\n",
              "         -2.5042, -7.4676, -8.7593, -9.2836, -7.3894, -8.7301, -6.3188, -3.8580,\n",
              "         -8.2070, -3.2706, -2.8383, -0.9641, -2.6980, -3.6532, -4.6353, -4.5463,\n",
              "         -4.4050, -4.3204, -4.2131, -4.2148, -4.2142, -4.1298, -3.9905, -3.8264,\n",
              "         -3.5232, -6.9396, -7.9711, -4.4820, -4.5442, -1.1332, -3.2390, -4.2630,\n",
              "         -4.9776, -5.1766, -5.1876, -5.0567, -4.8920, -4.6063, -4.4911, -4.2134,\n",
              "         -4.1927, -4.3144, -4.1008, -5.2510, -2.5150, -3.9526, -4.1521, -3.3720,\n",
              "          1.6891,  1.6047, -1.8248, -3.1511, -5.3778, -5.5570, -5.6295, -5.6106,\n",
              "         -5.5353, -5.4361, -5.3795, -5.1733, -4.9599, -4.9542, -3.9783, -6.6146,\n",
              "         -7.2114, -4.9308, -5.7816, -7.7141, -4.6133, -5.0679, -7.1435, -6.4658,\n",
              "         -2.5027,  0.1720, -3.2346, -4.6043, -6.2841, -6.3284, -6.2902, -6.2010,\n",
              "         -6.0829, -6.0498, -5.9472, -5.7552, -5.6840, -6.2972, -5.9652, -7.7507,\n",
              "         -5.8684, -6.9351, -7.4053, -6.6245, -3.2841, -0.8186, -3.6844, -5.1161,\n",
              "         -6.0732, -6.0203, -5.9566, -5.9033, -5.7522, -5.5633, -5.3288, -5.0366,\n",
              "         -4.8265, -4.5604, -4.8286, -9.3251, -5.2271, -5.8240, -7.7867, -9.4097,\n",
              "         -9.1235, -9.0894, -8.6467, -8.4671, -7.2334, -7.1090, -3.9972, -2.1819,\n",
              "         -3.8793, -4.9903, -5.9080, -5.8381, -5.7230, -5.6080, -5.5201, -5.4402,\n",
              "         -5.3387, -5.2084, -5.0015, -4.7936, -4.6053, -9.6256, -9.3992, -8.7648,\n",
              "         -9.1910, -8.9797, -9.2987, -8.7111, -9.0595, -9.0414, -9.2526, -6.9895,\n",
              "         -8.1131, -9.5833, -8.7854, -9.1024, -8.7688, -9.3769, -8.6856, -8.3744,\n",
              "         -6.6012, -9.0995, -9.0052, -6.3368, -2.8383, -8.3844]],\n",
              "       grad_fn=<CloneBackward0>), hidden_states=None, attentions=None)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "answer2_start_idx = torch.argmax(output2.start_logits)\n",
        "answer2_end_idx = torch.argmax(output2.end_logits)\n",
        "\n",
        "answer2_tokens = inputs2.input_ids[0, answer2_start_idx: answer2_end_idx + 1]\n",
        "answer2 = tokenizer.decode(answer2_tokens)\n",
        "print(\"question: {}\\nanswer: {}\".format(QA_input[2]['question'], answer2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J-J_tUdJq8I7",
        "outputId": "1da92aad-7bac-465c-8359-d7ca77769fd2"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "question: What are the key responsabilities?\n",
            "answer: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Method 2"
      ],
      "metadata": {
        "id": "M_-JhSrhlSGX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "qa = pipeline('question-answering', model=model_name, tokenizer=model_name)"
      ],
      "metadata": {
        "id": "Fm-4bdF2lTVe"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_0 = qa(QA_input[0]['question'], QA_input[0]['context'])\n",
        "print(output_0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wOuQoZVWlXRu",
        "outputId": "64a13225-06bb-4b77-c2a3-1f56fee47b41"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'score': 0.9752431511878967, 'start': 93, 'end': 95, 'answer': '13'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output_1 = qa(QA_input[1]['question'], QA_input[1]['context'])\n",
        "print(output_1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vbN_JK-UlaZb",
        "outputId": "08ba0d07-49f9-4249-8868-1ce5936b01fe"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'score': 0.21171429753303528, 'start': 59, 'end': 84, 'answer': 'gives freedom to the user'}\n"
          ]
        }
      ]
    }
  ]
}